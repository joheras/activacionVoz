{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activación por voz\n",
    "\n",
    "En este notebook se muestra cómo aplicar técnicas de aprendizaje profundo al reconocimiento de voz. En este trabajo, se construye un dataset de voz y se implementa un algoritmo para detectar una palabra de activación (en inglés _trigger word detection_). La detección de palabras de activación es la tecnología que permite a dispositvos como Amazon Alexa, Google Home, o a Apple Siri despertarse al oir cierta palabra. \n",
    "\n",
    "Para este ejercicio, trabajaremos con reconocimiento de voz en inglés debido a que por el momento no hay muchos datasets de reconocimiento de voz en español. Nuestra palabra de activación será \"_Activate_\". Cada vez que el programa oiga decir la palabra \"activate\", emitirá un sonido. Al final de este notebook, podrás grabar un audio con tu voy, y que el algoritmo se active y produzca un sonido cuando detecte decir \"activate\". \n",
    "\n",
    "Una vez que hayas terminado el notebook, este trabajo se puede extender para utilizar el algoritmo en tu ordenador y que cada vez que te oiga decir la palabra \"activate\" inicie una aplicación, lance algún evento, o que transcriba lo que hables (para esto último puede ser útil usar la [API de Google](https://towardsdatascience.com/auto-transcribe-google-speech-api-time-offsets-in-python-7c24c4db3720)).\n",
    "\n",
    "<img src=\"images/sound.png\" style=\"width:1000px;height:150px;\">\n",
    "\n",
    "En este notebook se explica cómo:\n",
    "- Estructurar un proyecto de reconocimiento de voz.\n",
    "- Sintentizar y procesar grabaciones de audio para crear datasets de entrenamiento y test.\n",
    "- Entrenar un modelo de detección de palabras de activación, y hacer predicciones con el.\n",
    "\n",
    "Este notebook está basado en el curso de [modelos secuenciales de Coursera](https://www.coursera.org/learn/nlp-sequence-models/home/welcome)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0 - Requisitos previos y carga de librerías\n",
    "\n",
    "Para trabajar con este notebook es necesario utilizar Python 3. Además necesitas instalar versiones concretas de las librerías keras y tensorflow. Para ello debes usar pip e instalar:\n",
    "`pip install tensorflow==1.2.1` y `pip install keras==2.0.7`. Lo más cómodo para gestionar diferentes versiones de Python es que utilicéis un [virtual environment](https://rukbottoland.com/blog/tutorial-de-python-virtualenv/). También es posible que necesites instalar otras librerías usando pip, pero para dichas librerías no hay problemas con las versiones. \n",
    "\n",
    "Una vez instaladas las librerías, cargamos los módulos que van a ser necesarios para esta primera parte del proyecto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pydub import AudioSegment\n",
    "import random\n",
    "import sys\n",
    "import io\n",
    "import os\n",
    "import glob\n",
    "import IPython\n",
    "from td_utils import *\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Síntesis de datos: creando un dataset de reconocimiento de voz\n",
    "\n",
    "Comenzamos creando un dataset para nuestro algoritmo de detección de palabras de activación. Un dataset de voz debería ser lo más cercano posible a la aplicación que queramos construir. En este caso, nos gustaría ser capaces de detectar la palabra \"activate\" en entornos variados (una biblioteca, en casa, la oficina, espacios abiertos, etc.) Por lo tanto es necesario crear grabaciones que mezclen palabras positivas (\"activate\") y negativas (otras palabras aleatóreas que sean distintas a \"activate\") con distintos sonidos de fondo. Veámos cómo se puede crear dicho dataset. \n",
    "\n",
    "### 1.1 - Escuchando a los datos\n",
    "\n",
    "Hemos conseguido una serie de grabaciones de sonidos de fondo en bibliotecas, cafeterías, restaurantes, casas, y oficinas; además de fragmentos de audio de distintas personas diciendo palabras positivas y negativas. El dataset incluye a personas hablando con una variedad de acentos. \n",
    "\n",
    "En la carpeta raw_data, puedes encontrar un subconjunto de los ficheros de audio de las palabras positivas, de las negativas y de los ruidos de fondo. Vamos a utilizar estos ficheros de audio para sintetizar un dataset para luego entrenar el modelo. El directorio \"activate\" contiene ejemplos positivos de personas diciendo la palaba \"activate\". El directorio \"negatives\" contiene ejemplos negativos de personas diciendo palabras aleatóreas distintas a \"activate\". Hay una palabra por cada grabación. La carpeta \"backgrounds\" contiene clips de 10 segundos de ruido de fondo en distintos entornos. \n",
    "\n",
    "Ejecuta la celda de abajo para oir alguno de estos ejemplos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IPython.display.Audio(\"./raw_data/activates/1.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IPython.display.Audio(\"./raw_data/negatives/4.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "IPython.display.Audio(\"./raw_data/backgrounds/1.wav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a usar estos tres tipos de grabaciones (positivos/negativos/fondo) para crear un dataset etiquetado. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 - De las grabaciones a los espectrogramas\n",
    "\n",
    "Antes de continuar, conviene explicar lo que es una grabación de audio. En una grabación, un micrófono graba pequeñas variaciones en la presión del aire a lo largo del tiempo, y son estas pequeñas variaciones en la presión del aire lo que nuestro oído percibe como sonido. Podemos pensar en una grabación de audio como una larga lista de números que miden los pequeños cambios de presión en el aire detectados por el micrófono. Nosotros usaremos audiosgrabados en la frecuencia 44100 Hz. Esto significa que el micrófono produce 44100 números por segundo. Por lo tanto, una grabación de 10 segundos se representa mediante 441000 números (= $10 \\times 44100$). \n",
    "\n",
    "A partir de esta representación \"pura\" o sin procesar de un audio es difícil detectar si la palabra \"activate\" fue dicha. Para facilitar que nuestro modelo aprenda de manera más sencilla las palabras de activación, vamos a calcular un *espectrograma* del audio. El espectrograma nos dice cuántas frecuencias diferentes están presentes en un fichero de audio en un momento de tiempo. \n",
    "\n",
    "(En caso de que hayáis estudiado algo sobre procesamiento de la señal o transformadas de Fourier, un espectrograma se calcula deslizando una ventana sobre una señal de audio y calculando las frecuencias más activas en cada ventana usando una transformada de Fourier. Si no entendéis la frase anterior, no os preocupéis, no es necesario).\n",
    "\n",
    "Veámos un ejemplo. Para ello vamos a cargar un audio y a continuación generamos su espectrograma.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IPython.display.Audio(\"audio_examples/example_train.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = graph_spectrogram(\"audio_examples/example_train.wav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El grafo anterior muestra cómo de activa es cada frecuencia (eje y) sobre un número de pasos de tiempo (eje x). \n",
    "\n",
    "<img src=\"images/spectrogram.png\" style=\"width:500px;height:200px;\">\n",
    "<center> **Figura 1**: Espectrograma de una grabación donde el color muestra el grado en el cual las diferentes frecuencias están presentes en distintos puntos en el tiempo. Los cuadrados verdes significan que una cierta frecuencia es más activa o está más presente en el audio; los cuadrados azules denotan frecuencias menos activas.</center>\n",
    "\n",
    "La dimensión de salida del espectrograma depende de los hiperparámetros del software de espectrograma y de la longitud de la entrada. En este notebook, trabajamos con ficheros de audio de 10 segundos como ficheros de tamaño estándar para nuestros ejemplos de entrenamiento. EL número de pasos en el tiempo será de 5511. Veremos más adelante que el espectrograma será la entrada $x$ de nuestra red."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, data = wavfile.read(\"audio_examples/example_train.wav\")\n",
    "print(\"Pasos de tiempo en la grabacion del audio antes del espectrograma\", data[:,0].shape)\n",
    "print(\"Pasos de tiempo en la entrada despues del espectrograma\", x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a definir dos variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tx = 5511 # El número de pasos de tiempo que servirán como entrada para el modelo.\n",
    "n_freq = 101 # Número de frecuencias de entrada para el modelo a cada paso de tiempo del espectrograma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notar que aunque utilizamos ficheros de 10 segundos como tamaño por defecto para nuestro conjunto de entrenamiento, 10 segundos de tiempo se pueden discretizar a distintos números de valores. Hemos visto que lo podemos discretizar a 441000 números usando el audio puro, y a 5511 números usando es espectrograma. En el primer caso, cada paso representa $10/441000 \\approx 0.000023$ segundos. En el segundo caso, cada paso representa $10/5511 \\approx 0.0018$ segundos. \n",
    "\n",
    "Para el audio de 10 segundos, los valores clave con los que vamos a trabajar son los siguientes:\n",
    "\n",
    "- $441000$ (audio puro)\n",
    "- $5511 = T_x$ (salida del espectrograma y dimensión de entrada de nuestro modelo). \n",
    "- $10000$ (usado por el módulo `pydub` para sintentizar audio) \n",
    "- $1375 = T_y$ (el número de pasos de la salida del modelo que vamos a construir). \n",
    "\n",
    "Notar que todas estas representaciones corresponden a exactamente 10 segundos de tiempo. La diferencia es que cada una discretiza esos 10 segundos de manera diferente. Estos valores se suelen usar como rangos estándar en la mayoría de sistemas de voz. \n",
    "\n",
    "Considerar el valor $T_y = 1375$. Esto significa que para la salida del modelo, discretizamos 10 segundos en 1375 intervalos de tiempo (cada uno de longitud $10/1375 \\approx 0.0072$s) y que intentamos predecir para cada uno de esos intervalos si alguien acaba de terminar de decir la palabra \"activate.\" \n",
    "\n",
    "Considerar ahora el valor 10000. Dicho valor corresponde a discretizar el fichero de 10 segundos en intervalos de tamaño 10/10000 = 0.001 segundos. 0.001 segundos es 1 milisegundo, or 1ms. Así que cuando discretizamos de acuerdo a intervalos de 1ms, significa que usamos 10000 pasos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ty = 1375 # Número de pasos en el modelo de salida"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 - Generando un único ejemplo de entrenamiento\n",
    "\n",
    "Debido a que la tarea de conseguir datos de voz y sus etiquetas es muy complicada, lo que vamos a hacer es sintetizar datos de entrenamiento usando ficheros de sonido de \"activates\", negativos y fondos. Es bastante lento grabar muchos ficheros de audio de 10 segundos con la palabra \"activate\" en entornos aleatoreos. Por el contrario, es mucho más sencillo grabar muchas veces palabras positivas y negativas, y grabar sonidos de fondo de manera separada (o descargar sonidos de fondo a partir de fuentes gratuitas y abiertas). \n",
    "\n",
    "Para sintentizar un único ejemplo de entrenamiento, vamos a :\n",
    "- Elegir un fichero aleatóreo de fondo de 10 segundos. \n",
    "- Insertar de manera aleatórea entre 0 y 4 ficheros de audio con la palabra \"activate\" en el fichero de 10 segundos.\n",
    "- Insertar de manera aleatórea entre 0 y 2 ficheros de audio con palabras negativas en el fichero de 10 segundos.\n",
    "\n",
    "Debido a que nosotros sintentizamos la palabra \"activate\" en el fichero de fondo, sabemos exáctamente en qué posición del fichero de 10 segundos aparece la palabra \"activate\". Esto hace sencillo generar la etiqueta $y^{\\langle t \\rangle}$.\n",
    "\n",
    "Para manipular el audio usaremos el módulo `pydub`. Pydub convierte ficheros de audio puro en listas de estructuras de datos de Pydub. Pydub usa 1ms como el intervalo de discretización por lo que un fichero de 10 segundos se representa con 10000 pasos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación cargamos segmentos de audio usando pydub. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activates, negatives, backgrounds = load_raw_audio()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver la longitud de ficheros de audio almacenados en las tres variables. En el caso de los ficheros de fondo la longitud debería ser 10000 debido a que es un audio de 10 segundos. Si mostramos la longitud de varios ficheros de activación, estas deberían estar en torno a 1000 ya que un audio de activación dura alrededor de 1 segundo, pero varía mucho dependiendo de quién lo diga. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"longitud fondo: \" + str(len(backgrounds[0])))    \n",
    "print(\"longitud activate[0]: \" + str(len(activates[0])))\n",
    "print(\"longitud activate[1]: \" + str(len(activates[1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Superponiendo palabras positivas/negativas en el fondo**:\n",
    "\n",
    "Dado un fichero de 10 segundos de fondo y un fichero corto (conteniendo una palabra negativa o positiva), es necesario ser capaz de añadir la palabra corta en el fichero de fondo. Para asegurarnos de que los segmentos insertados en el fondo no se superpongan es necesario tener en cuenta los tiempos en los que se han insertado previamente otros audios. Es decir, dado que vamos a insertar múltiples ficheros de palabras positivas/negativas en el fondo, no queremos que la palabra \"activate\" u otra palabra aleatórea se superponga con otra palabra añadida previamente. \n",
    "\n",
    "Es importante notar que cuando insertamos la palabra \"activate\" de 1 segundo en un fichero de 10 segundos de sonido de fondo en una cafetería, obtenemos un fichero que suena como alguien diciendo la palabra \"activate\" en una cafetería. Lo que *no* obtenemos es un fichero de 11 segundos. Veremos dentro de poco cómo pydub se encarga de todo esto. \n",
    "\n",
    "**Creando las etiquetas a la vez que la superposición**:\n",
    "\n",
    "Recordar que las etiquetas $y^{\\langle t \\rangle}$ representan si alguien acaba de decir la palabra \"activate\", es decir estamos en un problema de clasificación binaria. Dado un fichero de fondo, podemos inicializar $y^{\\langle t \\rangle}=0$ para todo $t$, dado que el fichero no contiene ningún \"activate.\" \n",
    "\n",
    "Al insertar una superposición de un fichero \"activate\", se debe actualizar la etiqueta para $y^{\\langle t \\rangle}$, de modo que los 50 pasos de la salida tengan ahora la etiqueta 1. Por ejemplo, suponer que hemos sintetizado un fichero donde se termina de decir \"activate\" en la marca de los 5 segundos (justo en medio del fichero). Recordar que  $T_y = 1375$, así que el paso de tiempo $687 = $ `int(1375*0.5)` corresponde con el momento de los 5 segundos en el audio. Así que fijamos, $y^{\\langle 688 \\rangle} = 1$. Además, queremos que el modelo sea capaz de detectar que \"activate\" se ha dicho en un periodo corto de tiempo después de este momento, por lo que fijamos los siguientes 50 valores de la etiqueta $y^{\\langle t \\rangle}$ a1. Específicamente tenemos que $y^{\\langle 688 \\rangle} = y^{\\langle 689 \\rangle} = \\cdots = y^{\\langle 737 \\rangle} = 1$.  \n",
    "\n",
    "Esta es otra razón para sintetizar los datos de entrenamiento: es relativamente directo generar las etiquetas $y^{\\langle t \\rangle}$ como acabamos de describir. Por el contrario, si tienes grabaciones de 10 segundos de un micrófono, marcar de manera manual cuándo se acaba de decir la palabra \"activate\" es una tarea manual que lleva mucho tiempo.\n",
    "\n",
    "A continuación se muestra una figura donde hemos ilustrado las etiquetas $y^{\\langle t \\rangle}$ para un fichero donde hemos insertado las palabras \"activate\", \"innocent\", activate\", y \"baby\". Notar que las etiquetas positivas (\"1\") están sólo asociadas con las palabras positivas.\n",
    "\n",
    "<img src=\"images/label_diagram.png\" style=\"width:500px;height:200px;\">\n",
    "<center> **Figura 2** </center>\n",
    "\n",
    "Para implementar proceso de síntesis del conjunto de entrenamiento se usan una serie de funciones auxiliares.  \n",
    "\n",
    "1. `get_random_time_segment(segment_ms)` obtiene un segmento de tiempo aleatóreo en nuestro audio de fondo.\n",
    "2. `is_overlapping(segment_time, existing_segments)` comprueba si un segmento de tiempo se superpone con otros segmentos existentes.\n",
    "3. `insert_audio_clip(background, audio_clip, existing_times)` inserta un segmento de audio en un momento elegido de manera aleatórea del audio de fondo usando `get_random_time_segment` y `is_overlapping`\n",
    "4. `insert_ones(y, segment_end_ms)` inserta 1s en el vector de etiquetas después de la palabra  \"activate\".\n",
    "\n",
    "Definimos dichas funciones a continuación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_time_segment(segment_ms):\n",
    "    \"\"\"\n",
    "    Gets a random time segment of duration segment_ms in a 10,000 ms audio clip.\n",
    "    \n",
    "    Arguments:\n",
    "    segment_ms -- the duration of the audio clip in ms (\"ms\" stands for \"milliseconds\")\n",
    "    \n",
    "    Returns:\n",
    "    segment_time -- a tuple of (segment_start, segment_end) in ms\n",
    "    \"\"\"\n",
    "    \n",
    "    segment_start = np.random.randint(low=0, high=10000-segment_ms)   \n",
    "    segment_end = segment_start + segment_ms - 1\n",
    "    \n",
    "    return (segment_start, segment_end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función `is_overlapping(segment_time, existing_segments)` consta de dos pasos:\n",
    "\n",
    "1. Crear un flag que se inicia a \"False\", y que posteriormente se cambia a \"True\" si se encuentra algún solapamiento. \n",
    "2. Se itera sobre los segmentos existentes (que vienen dados por pares (comienzo,fin)). Se comparan los tiempos con el comienzo y fin del segmento. Si hay algún solapamiento, el flag definido en (1) pasa a ser True. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_overlapping(segment_time, previous_segments):\n",
    "    \"\"\"\n",
    "    Checks if the time of a segment overlaps with the times of existing segments.\n",
    "    \n",
    "    Arguments:\n",
    "    segment_time -- a tuple of (segment_start, segment_end) for the new segment\n",
    "    previous_segments -- a list of tuples of (segment_start, segment_end) for the existing segments\n",
    "    \n",
    "    Returns:\n",
    "    True if the time segment overlaps with any of the existing segments, False otherwise\n",
    "    \"\"\"\n",
    "    \n",
    "    segment_start, segment_end = segment_time\n",
    "    \n",
    "    # Step 1: Initialize overlap as a \"False\" flag. (≈ 1 line)\n",
    "    overlap = False\n",
    "    \n",
    "    # Step 2: loop over the previous_segments start and end times.\n",
    "    # Compare start/end times and set the flag to True if there is an overlap (≈ 3 lines)\n",
    "    for previous_start, previous_end in previous_segments:\n",
    "        if  segment_start<=previous_end and segment_end >= previous_start:\n",
    "            overlap = True\n",
    "\n",
    "    return overlap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veámos algunos ejemplos del uso de esta función."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "overlap1 = is_overlapping((950, 1430), [(2000, 2550), (260, 949)])\n",
    "overlap2 = is_overlapping((2305, 2950), [(824, 1532), (1900, 2305), (3424, 3656)])\n",
    "print(\"Overlap 1 = \", overlap1)\n",
    "print(\"Overlap 2 = \", overlap2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función `insert_audio_clip()` permite insertar un nuevo fichero de audio en un fichero de ruido de fondo en una posición aleatórea pero comprobando que la inserción del segmento no se solapa con ninguna de las anteriores. Este proceso consta de cuatro pasos:\n",
    "1. Obtener un segmento de tiempo aleatóreo con la correcta duración.\n",
    "2. Comprobar que dicho segmento no se solapa con los segmentos previos. En caso de ser así, volver a 1. \n",
    "3. Añadir el nuevo segmento de tiempo a la lista de segmentos de tiempo añadidos. \n",
    "4. Superponer el clip de audio usando pydub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_audio_clip(background, audio_clip, previous_segments):\n",
    "    \"\"\"\n",
    "    Insert a new audio segment over the background noise at a random time step, ensuring that the \n",
    "    audio segment does not overlap with existing segments.\n",
    "    \n",
    "    Arguments:\n",
    "    background -- a 10 second background audio recording.  \n",
    "    audio_clip -- the audio clip to be inserted/overlaid. \n",
    "    previous_segments -- times where audio segments have already been placed\n",
    "    \n",
    "    Returns:\n",
    "    new_background -- the updated background audio\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get the duration of the audio clip in ms\n",
    "    segment_ms = len(audio_clip)\n",
    "    \n",
    "    # Step 1: Use one of the helper functions to pick a random time segment onto which to insert \n",
    "    # the new audio clip. (≈ 1 line)\n",
    "    segment_time = get_random_time_segment(segment_ms)\n",
    "    \n",
    "    # Step 2: Check if the new segment_time overlaps with one of the previous_segments. If so, keep \n",
    "    # picking new segment_time at random until it doesn't overlap. (≈ 2 lines)\n",
    "    while is_overlapping(segment_time,previous_segments):\n",
    "        segment_time = get_random_time_segment(segment_ms)\n",
    "\n",
    "    # Step 3: Add the new segment_time to the list of previous_segments (≈ 1 line)\n",
    "    previous_segments.append(segment_time)\n",
    "    \n",
    "    # Step 4: Superpose audio segment and background\n",
    "    new_background = background.overlay(audio_clip, position = segment_time[0])\n",
    "    \n",
    "    return new_background, segment_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veámos un ejemplo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(5)\n",
    "audio_clip, segment_time = insert_audio_clip(backgrounds[0], activates[0], [(3790, 4400)])\n",
    "audio_clip.export(\"insert_test.wav\", format=\"wav\")\n",
    "print(\"Segment Time: \", segment_time)\n",
    "IPython.display.Audio(\"insert_test.wav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, se proporciona la función `insert_ones()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_ones(y, segment_end_ms):\n",
    "    \"\"\"\n",
    "    Update the label vector y. The labels of the 50 output steps strictly after the end of the segment \n",
    "    should be set to 1. By strictly we mean that the label of segment_end_y should be 0 while, the\n",
    "    50 followinf labels should be ones.\n",
    "    \n",
    "    \n",
    "    Arguments:\n",
    "    y -- numpy array of shape (1, Ty), the labels of the training example\n",
    "    segment_end_ms -- the end time of the segment in ms\n",
    "    \n",
    "    Returns:\n",
    "    y -- updated labels\n",
    "    \"\"\"\n",
    "    \n",
    "    # duration of the background (in terms of spectrogram time-steps)\n",
    "    segment_end_y = int(segment_end_ms * Ty / 10000.0)\n",
    "    # Add 1 to the correct index in the background label (y)\n",
    "    for i in range(segment_end_y+1, segment_end_y+51):\n",
    "        if i < 1375:\n",
    "            y[0, i] = 1\n",
    "    \n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un ejemplo del uso de esta función se muestra a continuación. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr1 = insert_ones(np.zeros((1, Ty)), 9700)\n",
    "plt.plot(insert_ones(arr1, 4251)[0,:])\n",
    "print(\"sanity checks:\", arr1[0][1333], arr1[0][634], arr1[0][635])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ya por último, se define `create_training_example()` con los siguientes pasos:\n",
    "1. Inicializar el vector de etiquetas $y$ como un vector de ceros de numpy y de forma $(1, T_y)$.\n",
    "2. Inicializar el conjunto de segmentos existentes a cero.\n",
    "3. Seleccionar entre 0 y 4 ficheros de audio de \"activate\" e insertarlos en el fichero de ruído de fondo. Añadir también las etiquetas en la posición correcta del vector $y$.\n",
    "4. Elegir de manera aleatórea entre 0 y 2 audios negativos e insertarlos en el fichero de 10 segundos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_example(background, activates, negatives):\n",
    "    \"\"\"\n",
    "    Creates a training example with a given background, activates, and negatives.\n",
    "    \n",
    "    Arguments:\n",
    "    background -- a 10 second background audio recording\n",
    "    activates -- a list of audio segments of the word \"activate\"\n",
    "    negatives -- a list of audio segments of random words that are not \"activate\"\n",
    "    \n",
    "    Returns:\n",
    "    x -- the spectrogram of the training example\n",
    "    y -- the label at each time step of the spectrogram\n",
    "    \"\"\"\n",
    "    \n",
    "    # Set the random seed\n",
    "    np.random.seed(18)\n",
    "    \n",
    "    # Make background quieter\n",
    "    background = background - 20\n",
    "\n",
    "    # Step 1: Initialize y (label vector) of zeros (≈ 1 line)\n",
    "    y = np.zeros((1,Ty))\n",
    "\n",
    "    # Step 2: Initialize segment times as empty list (≈ 1 line)\n",
    "    previous_segments = list()\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Select 0-4 random \"activate\" audio clips from the entire list of \"activates\" recordings\n",
    "    number_of_activates = np.random.randint(0, 5)\n",
    "    random_indices = np.random.randint(len(activates), size=number_of_activates)\n",
    "    random_activates = [activates[i] for i in random_indices]\n",
    "    \n",
    "    # Step 3: Loop over randomly selected \"activate\" clips and insert in background\n",
    "    for random_activate in random_activates:\n",
    "        # Insert the audio clip on the background\n",
    "        background, segment_time = insert_audio_clip(background, random_activate, previous_segments)\n",
    "        # Retrieve segment_start and segment_end from segment_time\n",
    "        segment_start, segment_end = segment_time\n",
    "        # Insert labels in \"y\"\n",
    "        y = insert_ones(y,segment_end)\n",
    "\n",
    "    # Select 0-2 random negatives audio recordings from the entire list of \"negatives\" recordings\n",
    "    number_of_negatives = np.random.randint(0, 3)\n",
    "    random_indices = np.random.randint(len(negatives), size=number_of_negatives)\n",
    "    random_negatives = [negatives[i] for i in random_indices]\n",
    "\n",
    "    # Step 4: Loop over randomly selected negative clips and insert in background\n",
    "    for random_negative in random_negatives:\n",
    "        # Insert the audio clip on the background \n",
    "        background, _ = insert_audio_clip(background, random_negative, previous_segments)\n",
    "    \n",
    "    # Standardize the volume of the audio clip \n",
    "    background = match_target_amplitude(background, -20.0)\n",
    "\n",
    "    # Export new training example \n",
    "    file_handle = background.export(\"train\" + \".wav\", format=\"wav\")\n",
    "    print(\"File (train.wav) was saved in your directory.\")\n",
    "    \n",
    "    # Get and plot spectrogram of the new recording (background with superposition of positive and negatives)\n",
    "    x = graph_spectrogram(\"train.wav\")\n",
    "    \n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se muestra un ejemplo generado con la funcionalidad anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = create_training_example(backgrounds[0], activates, negatives)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora puedes escuchar el audio que se ha generado y compararlo con el espectrograma que se acaba de mostrar. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IPython.display.Audio(\"train.wav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, se pueden mostrar las etiquetas que se han generado para este ejemplo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 - Conjunto de entrenamiento completo\n",
    "\n",
    "El proceso que hemos descrito en el apartado 1.3 ha sido utilizado para generar un conjunto de entrenamiento completo lo suficientemente grante. Este conjunto de entrenamiento está guardado en los ficheros `.npy` de la carpeta `XY_train` y se pueden cargar con las siguientes instrucciones. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.load(\"./XY_train/X.npy\")\n",
    "Y = np.load(\"./XY_train/Y.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 - Conjunto de test\n",
    "\n",
    "Para probar nuestro modelo, tienes disponibles otros 25 ejemplos en la carpeta `XY_dev`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dev = np.load(\"./XY_dev/X_dev.npy\")\n",
    "Y_dev = np.load(\"./XY_dev/Y_dev.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Modelo\n",
    "\n",
    "Una vez que has construido el dataset, el siguiente paso consiste en construir el modelo de palabras de activación. Dicho modelo utiliza técnicas actuales de deep learning como son capas convolucionales 1-D, capas GRU y capas completamente densas (las únicas que hemos visto en el curso son las últimas que corresponden con las redes neuronales). Queda como taréa investigar un poco sobre estas capas.\n",
    "\n",
    "A continuación se muestra la arquitectura que se utiliza en este modelo. \n",
    "\n",
    "<img src=\"images/model.png\" style=\"width:600px;height:600px;\">\n",
    "<center> **Figura 3** </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Cargando el modelo pre-entrenado\n",
    "\n",
    "Debido a que entrenar un modelo de activación lleva mucho tiempo, se proporciona un modelo que ya ha sido previamente preentrenado utilizando la arquitectura anterior y utilizando un gran dataset de entrenamiento. Así que lo único que tenemos que hacer es cargar el modelo. Para ello es necesario cargar primero unas librerías."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model, load_model, Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, Input, Masking, TimeDistributed, LSTM, Conv1D\n",
    "from keras.layers import GRU, Bidirectional, BatchNormalization, Reshape\n",
    "from keras.optimizers import Adam\n",
    "import keras\n",
    "import h5py\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación cargamos el modelo y mostramos su estructura."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('./models/tr_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puedes continuar entrenando el modelo, por ejemplo utilizando el optimizador de Adam y usando como función de pérdida la función \"binary cross entropy\" del siguiente modo. Queda como taréa para vosotros investigar quiénes son ese optimizador y función de pérdida. Vamos a hacer un entrenamiento rápido con una única época y con un pequeño conjunto de entrenamiento de 26 muestras. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, decay=0.01)\n",
    "model.compile(loss='binary_crossentropy', optimizer=opt, metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X, Y, batch_size = 5, epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 - Probando el modelo\n",
    "\n",
    "Finálmente veámos como el modelo se comporta en el conjunto de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, acc = model.evaluate(X_dev, Y_dev)\n",
    "print(\"Dev set accuracy = \", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esto parece un buen resultado (93% de acierto). En este caso la accuracy no es la mejor métrica ya que la mayoría de las etiquetas tienden a ser 0s por lo que una red que prediga siempre 0 tendrá una accuracy bastante alta. Podéis probar a definir métricas más útiles como la métrica F1 o la Precission/Recall. Pero por el momento no nos preocupamos por eso y vamos a probar de manera empírica qué tal funciona el modelo. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Haciendo predicciones\n",
    "\n",
    "Ahora que tenemos un modelo que funciona para la detección de palabras de activación, vamos a usarlo para hacer predicciones. El siguiente código sirve para hacer predicciones en un fichero de audio. La función `predict_activates()` hace lo siguiente:\n",
    "\n",
    "1. Calcula el espectrograma del fichero de audio.\n",
    "2. Usa `np.swap` y `np.expand_dims` para preparar la entrada a la red. \n",
    "3. Se usa propagación hacia adelante en el modelo para calcular las predicciones en cada paso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_triggerword(filename):\n",
    "    plt.subplot(2, 1, 1)\n",
    "\n",
    "    x = graph_spectrogram(filename)\n",
    "    # the spectogram outputs (freqs, Tx) and we want (Tx, freqs) to input into the model\n",
    "    x  = x.swapaxes(0,1)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    predictions = model.predict(x)\n",
    "    \n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(predictions[0,:,0])\n",
    "    plt.ylabel('probability')\n",
    "    plt.show()\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez que se ha estimado la probabilidad de haber detectado la palabra \"activate\" en cada paso, podemos producir un sonido de campana cuando la probabilidad supera un cierto margen. Para ello implementamos el siguiente proceso:\n",
    "\n",
    "1. Se itera sobre las probabilidades. \n",
    "2. Cuando la probabilidad es superior a 75 se reproduce un sonido de campana."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chime_file = \"./audio_examples/chime.wav\"\n",
    "def chime_on_activate(filename, predictions, threshold):\n",
    "    audio_clip = AudioSegment.from_wav(filename)\n",
    "    chime = AudioSegment.from_wav(chime_file)\n",
    "    Ty = predictions.shape[1]\n",
    "    # Step 1: Initialize the number of consecutive output steps to 0\n",
    "    consecutive_timesteps = 0\n",
    "    # Step 2: Loop over the output steps in the y\n",
    "    for i in range(Ty):\n",
    "        # Step 3: Increment consecutive output steps\n",
    "        consecutive_timesteps += 1\n",
    "        # Step 4: If prediction is higher than the threshold and more than 75 consecutive output steps have passed\n",
    "        if predictions[0,i,0] > threshold and consecutive_timesteps > 75:\n",
    "            # Step 5: Superpose audio and background using pydub\n",
    "            audio_clip = audio_clip.overlay(chime, position = ((i / Ty) * audio_clip.duration_seconds)*1000)\n",
    "            # Step 6: Reset consecutive output steps to 0\n",
    "            consecutive_timesteps = 0\n",
    "        \n",
    "    audio_clip.export(\"chime_output.wav\", format='wav')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 - Probando en ejemplos de test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veámos como nuestro modelo funciona en dos ficheros de audio que no han sido utilizados previamente. Primero cargamos y oímos dichos ficheros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IPython.display.Audio(\"./raw_data/dev/1.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IPython.display.Audio(\"./raw_data/dev/2.wav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora vamos a utilizar el modelo para incluir el sonido de campana cada vez que se oiga la palabra \"activate\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"./raw_data/dev/1.wav\"\n",
    "prediction = detect_triggerword(filename)\n",
    "chime_on_activate(filename, prediction, 0.5)\n",
    "IPython.display.Audio(\"./chime_output.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename  = \"./raw_data/dev/2.wav\"\n",
    "prediction = detect_triggerword(filename)\n",
    "chime_on_activate(filename, prediction, 0.5)\n",
    "IPython.display.Audio(\"./chime_output.wav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Prueba con otros ejemplos\n",
    "\n",
    "En esta última parte se puede probar el modelo con ficheros creados por vosotros mismos. Graba un fichero de audio de 10 segundos donde digas la palabra \"activate\" y otras palabras aleatoréas. A continuación añade dicho fichero a una carpeta del proyecto. El fichero tiene que estar en formato wav. Si el fichero de audio tiene más o menos de 10 segundos, el siguiente código sirve para recortarlo o ampliarlo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the audio to the correct format\n",
    "def preprocess_audio(filename):\n",
    "    # Trim or pad audio segment to 10000ms\n",
    "    padding = AudioSegment.silent(duration=10000)\n",
    "    segment = AudioSegment.from_wav(filename)[:10000]\n",
    "    segment = padding.overlay(segment)\n",
    "    # Set frame rate to 44100\n",
    "    segment = segment.set_frame_rate(44100)\n",
    "    # Export as wav\n",
    "    segment.export(filename, format='wav')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez añadido el fichero al proyecto, indica a continuación el path y comprueba que puedes oir dicho fichero. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "your_filename = \"audio_examples/my_audio.wav\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_audio(your_filename)\n",
    "IPython.display.Audio(your_filename) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, usa el modelo para predecir dondé habéis dicho \"activate\". Si no se produce el sonido de campanas es posible que necesites modificar el valor de threshold. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chime_threshold = 0.5\n",
    "prediction = detect_triggerword(your_filename)\n",
    "chime_on_activate(your_filename, prediction, chime_threshold)\n",
    "IPython.display.Audio(\"./chime_output.wav\")"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "nlp-sequence-models",
   "graded_item_id": "rSupZ",
   "launcher_item_id": "cvGhe"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
